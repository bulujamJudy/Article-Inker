{"backend_state":"running","connection_file":"/projects/0ddeade5-3577-4fe8-8cd6-8a0cb653428e/.local/share/jupyter/runtime/kernel-76a7fd52-40a6-446a-9176-cc8253aabd83.json","kernel":"nlp_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Seq2Seq en-cn.ipynb","provenance":[]},"interpreter":{"hash":"335ee12212264728feb72f243af72c5a8ea26c832f07e1f651ce9e17c7ceae23"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1655233024334,"exec_count":1,"id":"e2b5c9","input":"import numpy as np\nimport torch.nn as nn\nfrom nltk.tokenize import WordPunctTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport pandas as pd\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfrom Word2Sequence import Word2Sequence\nfrom Dataset import Dataset\nfrom Seq2Seq import Seq2Seq","kernel":"nlp_env","metadata":{"id":"PS0kPzE04YFO"},"no_halt":true,"pos":2,"start":1655233020975,"state":"done","type":"cell"}
{"cell_type":"code","end":1655233024388,"exec_count":2,"id":"de1e41","input":"# read small_en-cn.txt file\ndata_path = './eng-chin.txt'\ndf = pd.read_table(data_path,header=None).iloc[:,:]\ndf = df.drop([2],axis=1)\ndf.columns=['english','chinese']\n\ninput_texts = df.english.values.tolist() #this will be all of the english sentences\ntarget_texts = df.chinese.values.tolist() #this will be all of the chinese sentences","kernel":"nlp_env","no_halt":true,"pos":3,"start":1655233024363,"state":"done","type":"cell"}
{"cell_type":"code","end":1655233024427,"exec_count":3,"id":"b3f367","input":"'''\nYour code here: Try printing some english and chinese sentences from their lists input_texts and target_texts!\n'''\nprint(target_texts)","kernel":"nlp_env","no_halt":true,"output":{"0":{"name":"stdout","text":"['有没有一个国家比美国更提倡爱国主义？', '有空的时候，我总喜欢听古典音乐。', '我九岁的时候问我妈妈圣诞老人是否真的存在。', '我是外国人，我捷克语不好，请说慢一点。', '如果可能的話, 我希望你參加下一次的會議。', '如果你喜歡你做的工作，你就有比金錢更有價值的東西。', '直到我自己有了孩子我才明白了什么是母爱。', '善良是聾子能聽盲人能看的語言。', '玛丽哭着从学校跑回了家里，因为她的朋友捉弄了她。', '我爸爸对我的爱和照顾不比我妈妈少。', '现在是你决定是不是真要结婚的时候。', '人會因為體溫而在紅外線攝影機上顯現。', '警察大概从一个月前就开始找被偷物品了。', '他們告訴我吃完這個藥我就會覺得舒服一點。', '因为家里钱不够所以汤姆没能念大学。', '他的食物供给不足的的时候，他不得不去找新的地方居住。', '我昨晚在床上看书的时候点着灯就睡了。', '“你去哪儿了？”“我去了火车站送我的一个朋友。”', '对了，前些时间你说伞不见了，现在找到了吗？', '當她看到媽媽沒在生她的氣，她的雙眼因為幸福而閃爍了。', '我不用曲膝就能把我的手掌放到地上。', '我十八歲時，學了開車、考到了駕照。', '我十八歲時，學了開車、考到了駕照。', '我本来预备今天去海滩的，但接着天就开始下雨了。', '如果我们知道我们在做什么，那么这不能称之为研究，是吗？', '如果你想去，就去好了。如果你不想去，那也没什么大不了的。', '在美国，大多数人能在十八岁后投票选举。', '得不到的东西就最想得到。', '比起少抽菸，你何不直接把菸戒了？', '反社会者极少为他们的罪行显露懊悔或有罪恶的感觉。', '汤姆不能说明玛丽遇害那天自己在哪里。', '汤姆从没忘记在婚礼周年纪念日送给他妻子花。', '在巴黎，沒有人能夠理解湯姆的法文。', '我们那时在谈论事情，但我不记得是什么了。', '你不应该在社交网络上分享过多私人信息。', '湯姆失業後，為了排遣鬱悶的心情而開始了賭博。', '放學後，我到一所英語學校去練習英語會話。', '我想你要拿到驾照根本不难。', '我以为我们发现了绝妙的藏身之处，但警察找到了我们。', '我想知道最近的美國運通辦事處的電話號碼。', '她定期去看牙医，所以她很少牙痛。', '让事情更糟糕的是，他没有注意到他打扰到了邻居。', '你似乎對來自國外的想法有偏見。', '如果一個病人折一千隻紙鶴, 她的願望就會成真。', '真难相信汤姆不知道玛丽爱他。', '汤姆在夏休回乡看望父母。', '筹划旅行的时候，我们必须考虑到全家人的意愿。', '如果你不会说英语，你就很难得到一个好的职位。', '我到櫃檯拿了鑰匙，然後就乘電梯去了我房間的樓層。', '英语现已成为世界上许多国家的通用语言了。', '在校外，她见到没有家的人们住在纸板箱里。', '總理的發言估計激怒了在野黨。', '这电子辞典的好处就是便于携带。', '當這位女士得知她已經贏得了百萬美元, 她真的樂瘋了。', '我们对这次的延迟表示抱歉，并对可能造成的不便表示遗憾。', '根據報載，有一架飛機昨天晚上發生了意外。', '從他大學畢業以後, 他教了兩年的英語。', '如果一个人有11只羊，除了9只之外，其他全部死了，那么他还剩下几只羊呢？', '如果那天下雨，比赛会顺延至下一个晴天。', '给你解释这为什么行不通要花很多时间。', '该国严禁进口稀有野生动物。', '忠犬八公的雕像伫立在涩谷站前。', '每一個人對事情的看法不同是依據他們是富有還是貧窮。', '尽管政府拒绝承认，它的经济政策还是失败了。', 'Mary试图把围裙围在腰上，然后把烤鸡从炉子里拿出来。', '人們看待事情的角度不同取決於他們是富裕或貧窮。', '倫敦的人口遠遠多於其他英國城市的人口。', '他们认为，去反驳一个不认识的人有些不礼貌。', '四分之三的美国人相信存在超自然现象。', '汤姆得出无论他做什么，玛丽都不会喜欢的结论。', '全世界百分之八十電腦上的資訊都是用英語寫的。', '我以为我们发现了绝妙的藏身之处，但警察找到了我们。', '早上起来，嗓子变得很沙哑，我想是不是感冒了。', '如果看起来像个苹果而且吃起来也像苹果的话，可能就是苹果。', '世界就像是一本书，走一步等于翻了一页。', '今天，我本打算在图书馆学习但到12点左右才醒。', 'Tom写作可以写的像本国人一样，可是他的发音很烂', '汤姆尽了全力，但他还是不能获得比玛丽更高的等级。', '当列车停止时，所有的乘客都想知道发生了什么。', 'Charles Lindbergh於1927年成功完成了第一次獨自飛越大西洋。', '他的分数总比我高，尽管他学习得少一点。', '其实我工作并不多，但足以让我这周在办公室里忙着了。', '我还了从图书馆借的书，又借了些新的。', '文章的发表被预定在教授生日那天。', '她受歡迎不是因為她的美麗，而是因為她親切地對待每個人。', '电话运营商提示来电人等候接通。', '那些说钱不能买来幸福的人，只是不知道上哪里去买而已。', '當時沒有任何以英語為母語的人在公立學校任教。', '如果你远离危险区域，里约热内卢就是完全安全的。', '她被要求去说服他以让他或者他的儿子或者是别的人来粉刷屋子。', '汤姆总是说话声音太小，我几乎听不懂他在说什么。', '尽管我在学校学了6年英语，我还是说不好。', '我为自己买了副羽毛球拍，但我忘记买羽毛球了。', '我不知道这个城市，而且我一点都不懂那里的语言。', '两个孩子的年龄加起来和他们的父亲相当。', '对工具箱里只有一把榔头的人来说，所有的问题都像钉子。', '汤姆去河里游泳，但当他出来时，他的衣服被偷了。', '作為一個良好的交談者，並不只意味著作一個英語說得好的說話者。', '人孔是圓的，因為這樣人孔蓋就不會意外地掉進洞裡。']\n"}},"pos":4,"start":1655233024402,"state":"done","type":"cell"}
{"cell_type":"code","end":1655233024577,"exec_count":4,"id":"526087","input":"#read in our model object. Tokenize our data\ntk = WordPunctTokenizer()\nenglish = [tk.tokenize(sentence.lower()) for sentence in input_texts]\nchinese = [[x for x in sentence] for sentence in target_texts]\nenglish","kernel":"nlp_env","metadata":{"id":"cPXhq9PO4dOC"},"no_halt":true,"output":{"0":{"data":{"text/plain":"[['does',\n  'any',\n  'other',\n  'country',\n  'fan',\n  'the',\n  'flames',\n  'of',\n  'patriotism',\n  'as',\n  'much',\n  'as',\n  'america',\n  '?'],\n ['i',\n  'always',\n  'enjoy',\n  'listening',\n  'to',\n  'classical',\n  'music',\n  'when',\n  'i',\n  'have',\n  'some',\n  'free',\n  'time',\n  '.'],\n ['i',\n  'was',\n  'nine',\n  'years',\n  'old',\n  'when',\n  'i',\n  'asked',\n  'my',\n  'mom',\n  'if',\n  'santa',\n  'claus',\n  'really',\n  'existed',\n  '.'],\n ['i',\n  \"'\",\n  'm',\n  'a',\n  'foreigner',\n  'and',\n  'i',\n  'don',\n  \"'\",\n  't',\n  'know',\n  'czech',\n  'very',\n  'well',\n  '.',\n  'please',\n  ',',\n  'speak',\n  'slowly',\n  '.'],\n ['if',\n  'it',\n  \"'\",\n  's',\n  'at',\n  'all',\n  'possible',\n  ',',\n  'i',\n  \"'\",\n  'd',\n  'like',\n  'you',\n  'to',\n  'take',\n  'part',\n  'in',\n  'the',\n  'next',\n  'meeting',\n  '.'],\n ['if',\n  'you',\n  'enjoy',\n  'the',\n  'work',\n  'you',\n  'do',\n  ',',\n  'you',\n  'have',\n  'something',\n  'worth',\n  'more',\n  'than',\n  'money',\n  '.'],\n ['it',\n  'was',\n  'not',\n  'until',\n  'i',\n  'had',\n  'a',\n  'baby',\n  'myself',\n  'that',\n  'i',\n  'knew',\n  'what',\n  'mother',\n  \"'\",\n  's',\n  'love',\n  'is',\n  '.'],\n ['kindness',\n  'is',\n  'the',\n  'language',\n  'which',\n  'the',\n  'deaf',\n  'can',\n  'hear',\n  'and',\n  'the',\n  'blind',\n  'can',\n  'see',\n  '.'],\n ['mary',\n  'came',\n  'home',\n  'from',\n  'school',\n  'in',\n  'tears',\n  'because',\n  'her',\n  'friends',\n  'had',\n  'teased',\n  'her',\n  '.'],\n ['my',\n  'father',\n  'was',\n  'no',\n  'less',\n  'affectionate',\n  'and',\n  'tender',\n  'to',\n  'me',\n  'than',\n  'my',\n  'mother',\n  'was',\n  '.'],\n ['now',\n  \"'\",\n  's',\n  'the',\n  'time',\n  'to',\n  'decide',\n  'whether',\n  'you',\n  'really',\n  'want',\n  'to',\n  'get',\n  'married',\n  'or',\n  'not',\n  '.'],\n ['people',\n  'show',\n  'up',\n  'bright',\n  'on',\n  'an',\n  'infrared',\n  'camera',\n  'because',\n  'of',\n  'their',\n  'body',\n  'heat',\n  '.'],\n ['the',\n  'police',\n  'have',\n  'been',\n  'searching',\n  'for',\n  'the',\n  'stolen',\n  'goods',\n  'for',\n  'almost',\n  'a',\n  'month',\n  '.'],\n ['they',\n  'told',\n  'me',\n  'that',\n  'i',\n  'would',\n  'feel',\n  'a',\n  'little',\n  'better',\n  'if',\n  'i',\n  'took',\n  'this',\n  'medicine',\n  '.'],\n ['tom',\n  'couldn',\n  \"'\",\n  't',\n  'go',\n  'to',\n  'college',\n  'because',\n  'his',\n  'family',\n  'didn',\n  \"'\",\n  't',\n  'have',\n  'enough',\n  'money',\n  '.'],\n ['when',\n  'his',\n  'food',\n  'supply',\n  'ran',\n  'short',\n  ',',\n  'he',\n  'had',\n  'to',\n  'look',\n  'for',\n  'a',\n  'new',\n  'place',\n  'to',\n  'live',\n  '.'],\n ['while',\n  'i',\n  'was',\n  'reading',\n  'in',\n  'bed',\n  'last',\n  'night',\n  ',',\n  'i',\n  'fell',\n  'asleep',\n  'with',\n  'the',\n  'light',\n  'on',\n  '.'],\n ['where',\n  'have',\n  'you',\n  'been',\n  '?',\n  '\"',\n  'i',\n  'have',\n  'been',\n  'to',\n  'the',\n  'station',\n  'to',\n  'see',\n  'a',\n  'friend',\n  'off',\n  '.\"'],\n ['by',\n  'the',\n  'way',\n  ',',\n  'did',\n  'you',\n  'find',\n  'the',\n  'umbrella',\n  'you',\n  'said',\n  'you',\n  \"'\",\n  'd',\n  'lost',\n  'the',\n  'other',\n  'day',\n  '?'],\n ['her',\n  'eyes',\n  'shone',\n  'with',\n  'joy',\n  'when',\n  'she',\n  'saw',\n  'that',\n  'her',\n  'mother',\n  'was',\n  'not',\n  'mad',\n  'at',\n  'her',\n  '.'],\n ['i',\n  'can',\n  'place',\n  'the',\n  'palms',\n  'of',\n  'my',\n  'hands',\n  'on',\n  'the',\n  'floor',\n  'without',\n  'bending',\n  'my',\n  'knees',\n  '.'],\n ['i',\n  'learned',\n  'to',\n  'drive',\n  'a',\n  'car',\n  'and',\n  'got',\n  'a',\n  'driver',\n  \"'\",\n  's',\n  'license',\n  'when',\n  'i',\n  'was',\n  'eighteen',\n  '.'],\n ['i',\n  'learned',\n  'to',\n  'drive',\n  'a',\n  'car',\n  'when',\n  'i',\n  'was',\n  'eighteen',\n  'and',\n  'got',\n  'a',\n  'driver',\n  \"'\",\n  's',\n  'license',\n  '.'],\n ['i',\n  'was',\n  'planning',\n  'on',\n  'going',\n  'to',\n  'the',\n  'beach',\n  'today',\n  ',',\n  'but',\n  'then',\n  'it',\n  'started',\n  'to',\n  'rain',\n  '.'],\n ['if',\n  'we',\n  'knew',\n  'what',\n  'we',\n  'were',\n  'doing',\n  ',',\n  'it',\n  'wouldn',\n  \"'\",\n  't',\n  'be',\n  'called',\n  'research',\n  ',',\n  'would',\n  'it',\n  '?'],\n ['if',\n  'you',\n  'want',\n  'to',\n  'go',\n  ',',\n  'then',\n  'go',\n  '.',\n  'if',\n  'you',\n  'don',\n  \"'\",\n  't',\n  'want',\n  'to',\n  ',',\n  'then',\n  'it',\n  \"'\",\n  's',\n  'no',\n  'big',\n  'deal',\n  '.'],\n ['in',\n  'the',\n  'u',\n  '.',\n  's',\n  '.,',\n  'most',\n  'people',\n  'can',\n  'vote',\n  'when',\n  'they',\n  'reach',\n  'eighteen',\n  'years',\n  'of',\n  'age',\n  '.'],\n ['it',\n  'is',\n  'the',\n  'things',\n  'that',\n  'we',\n  'do',\n  'not',\n  'possess',\n  'which',\n  'seem',\n  'to',\n  'us',\n  'most',\n  'desirable',\n  '.'],\n ['rather',\n  'than',\n  'cutting',\n  'down',\n  'on',\n  'cigarettes',\n  ',',\n  'why',\n  'don',\n  \"'\",\n  't',\n  'you',\n  'just',\n  'give',\n  'them',\n  'up',\n  '?'],\n ['sociopaths',\n  'rarely',\n  'display',\n  'remorse',\n  'or',\n  'feelings',\n  'of',\n  'guilt',\n  'for',\n  'their',\n  'crimes',\n  '.'],\n ['tom',\n  'can',\n  \"'\",\n  't',\n  'account',\n  'for',\n  'his',\n  'whereabouts',\n  'on',\n  'the',\n  'day',\n  'that',\n  'mary',\n  'was',\n  'murdered',\n  '.'],\n ['tom',\n  'never',\n  'forgets',\n  'to',\n  'give',\n  'his',\n  'wife',\n  'flowers',\n  'on',\n  'their',\n  'wedding',\n  'anniversary',\n  '.'],\n ['tom',\n  'was',\n  'able',\n  'to',\n  'make',\n  'himself',\n  'understood',\n  'in',\n  'french',\n  'when',\n  'he',\n  'visited',\n  'paris',\n  '.'],\n ['we',\n  'were',\n  'talking',\n  'about',\n  'something',\n  'at',\n  'that',\n  'time',\n  ',',\n  'but',\n  'i',\n  'don',\n  \"'\",\n  't',\n  'remember',\n  'what',\n  '.'],\n ['you',\n  'shouldn',\n  \"'\",\n  't',\n  'share',\n  'too',\n  'much',\n  'private',\n  'information',\n  'on',\n  'the',\n  'social',\n  'networks',\n  '.'],\n ['after',\n  'tom',\n  'lost',\n  'his',\n  'job',\n  ',',\n  'he',\n  'started',\n  'to',\n  'gamble',\n  'to',\n  'cope',\n  'with',\n  'his',\n  'depression',\n  '.'],\n ['after',\n  'school',\n  ',',\n  'i',\n  'go',\n  'to',\n  'an',\n  'english',\n  'school',\n  'to',\n  'practice',\n  'english',\n  'conversation',\n  '.'],\n ['i',\n  'think',\n  'you',\n  \"'\",\n  'll',\n  'have',\n  'very',\n  'little',\n  'difficulty',\n  'in',\n  'getting',\n  'a',\n  'driver',\n  \"'\",\n  's',\n  'license',\n  '.'],\n ['i',\n  'thought',\n  'we',\n  'had',\n  'found',\n  'the',\n  'perfect',\n  'hiding',\n  'place',\n  ',',\n  'but',\n  'the',\n  'police',\n  'found',\n  'us',\n  '.'],\n ['i',\n  \"'\",\n  'd',\n  'like',\n  'to',\n  'know',\n  'the',\n  'phone',\n  'number',\n  'of',\n  'the',\n  'nearest',\n  'american',\n  'express',\n  'office',\n  '.'],\n ['she',\n  'visits',\n  'the',\n  'dentist',\n  'on',\n  'a',\n  'regular',\n  'basis',\n  ',',\n  'so',\n  'she',\n  'seldom',\n  'gets',\n  'toothaches',\n  '.'],\n ['to',\n  'make',\n  'matters',\n  'worse',\n  ',',\n  'he',\n  'isn',\n  \"'\",\n  't',\n  'even',\n  'conscious',\n  'of',\n  'annoying',\n  'his',\n  'neighbors',\n  '.'],\n ['you',\n  'seem',\n  'to',\n  'be',\n  'prejudiced',\n  'against',\n  'ideas',\n  'that',\n  'come',\n  'from',\n  'foreign',\n  'countries',\n  '.'],\n ['if',\n  'a',\n  'sick',\n  'person',\n  'folds',\n  'one',\n  'thousand',\n  'paper',\n  'cranes',\n  ',',\n  'her',\n  'wish',\n  'will',\n  'come',\n  'true',\n  '.'],\n ['it',\n  \"'\",\n  's',\n  'hard',\n  'to',\n  'believe',\n  'that',\n  'tom',\n  'wasn',\n  \"'\",\n  't',\n  'aware',\n  'that',\n  'mary',\n  'was',\n  'in',\n  'love',\n  'with',\n  'him',\n  '.'],\n ['tom',\n  'returned',\n  'to',\n  'his',\n  'hometown',\n  'to',\n  'visit',\n  'his',\n  'parents',\n  'during',\n  'the',\n  'summer',\n  'break',\n  '.'],\n ['we',\n  'must',\n  'take',\n  'into',\n  'account',\n  'the',\n  'wishes',\n  'of',\n  'all',\n  'the',\n  'family',\n  'in',\n  'planning',\n  'a',\n  'trip',\n  '.'],\n ['you',\n  \"'\",\n  're',\n  'much',\n  'less',\n  'likely',\n  'to',\n  'get',\n  'a',\n  'good',\n  'position',\n  'if',\n  'you',\n  'don',\n  \"'\",\n  't',\n  'speak',\n  'english',\n  '.'],\n ['after',\n  'asking',\n  'for',\n  'my',\n  'key',\n  'at',\n  'the',\n  'front',\n  'desk',\n  ',',\n  'i',\n  'took',\n  'the',\n  'elevator',\n  'to',\n  'my',\n  'floor',\n  '.'],\n ['english',\n  'has',\n  'now',\n  'become',\n  'the',\n  'common',\n  'language',\n  'of',\n  'several',\n  'nations',\n  'in',\n  'the',\n  'world',\n  '.'],\n ['outside',\n  'the',\n  'school',\n  ',',\n  'she',\n  'saw',\n  'people',\n  'with',\n  'no',\n  'homes',\n  'living',\n  'in',\n  'cardboard',\n  'boxes',\n  '.'],\n ['the',\n  'prime',\n  'minister',\n  \"'\",\n  's',\n  'speech',\n  'was',\n  'calculated',\n  'to',\n  'anger',\n  'the',\n  'opposition',\n  'parties',\n  '.'],\n ['the',\n  'good',\n  'thing',\n  'about',\n  'this',\n  'electronic',\n  'dictionary',\n  'is',\n  'that',\n  'it',\n  \"'\",\n  's',\n  'easy',\n  'to',\n  'carry',\n  '.'],\n ['the',\n  'lady',\n  'really',\n  'flipped',\n  'out',\n  'when',\n  'she',\n  'learned',\n  'she',\n  'had',\n  'won',\n  'a',\n  'million',\n  'dollars',\n  '.'],\n ['we',\n  'apologize',\n  'for',\n  'the',\n  'delay',\n  'and',\n  'regret',\n  'any',\n  'inconvenience',\n  'it',\n  'may',\n  'have',\n  'caused',\n  '.'],\n ['according',\n  'to',\n  'newspaper',\n  'reports',\n  ',',\n  'there',\n  'was',\n  'an',\n  'airplane',\n  'accident',\n  'last',\n  'evening',\n  '.'],\n ['after',\n  'he',\n  'had',\n  'graduated',\n  'from',\n  'the',\n  'university',\n  ',',\n  'he',\n  'taught',\n  'english',\n  'for',\n  'two',\n  'years',\n  '.'],\n ['if',\n  'a',\n  'man',\n  'had',\n  '11',\n  'sheep',\n  'and',\n  'all',\n  'but',\n  '9',\n  'died',\n  ',',\n  'how',\n  'many',\n  'sheep',\n  'would',\n  'he',\n  'have',\n  'left',\n  '?'],\n ['if',\n  'it',\n  'rains',\n  'on',\n  'that',\n  'day',\n  ',',\n  'the',\n  'game',\n  'will',\n  'be',\n  'postponed',\n  'until',\n  'the',\n  'next',\n  'fine',\n  'day',\n  '.'],\n ['it',\n  'would',\n  'take',\n  'me',\n  'too',\n  'much',\n  'time',\n  'to',\n  'explain',\n  'to',\n  'you',\n  'why',\n  'it',\n  \"'\",\n  's',\n  'not',\n  'going',\n  'to',\n  'work',\n  '.'],\n ['the',\n  'importation',\n  'of',\n  'rare',\n  'wild',\n  'animals',\n  'to',\n  'this',\n  'country',\n  'is',\n  'strictly',\n  'prohibited',\n  '.'],\n ['the',\n  'statue',\n  'of',\n  'hachiko',\n  ',',\n  'the',\n  'faithful',\n  'dog',\n  ',',\n  'stands',\n  'in',\n  'front',\n  'of',\n  'shibuya',\n  'station',\n  '.'],\n ['a',\n  'person',\n  'views',\n  'things',\n  'differently',\n  'according',\n  'to',\n  'whether',\n  'they',\n  'are',\n  'rich',\n  'or',\n  'poor',\n  '.'],\n ['although',\n  'the',\n  'government',\n  'refuses',\n  'to',\n  'admit',\n  'it',\n  ',',\n  'its',\n  'economic',\n  'policy',\n  'is',\n  'in',\n  'ruins',\n  '.'],\n ['mary',\n  'tied',\n  'an',\n  'apron',\n  'around',\n  'her',\n  'waist',\n  'and',\n  'then',\n  'took',\n  'the',\n  'turkey',\n  'out',\n  'of',\n  'the',\n  'oven',\n  '.'],\n ['people',\n  'look',\n  'at',\n  'things',\n  'differently',\n  'depending',\n  'on',\n  'whether',\n  'they',\n  'are',\n  'rich',\n  'or',\n  'poor',\n  '.'],\n ['the',\n  'population',\n  'of',\n  'london',\n  'is',\n  'much',\n  'greater',\n  'than',\n  'that',\n  'of',\n  'any',\n  'other',\n  'british',\n  'city',\n  '.'],\n ['they',\n  'consider',\n  'it',\n  'impolite',\n  'to',\n  'disagree',\n  'with',\n  'someone',\n  'they',\n  'don',\n  \"'\",\n  't',\n  'know',\n  'very',\n  'well',\n  '.'],\n ['three',\n  'out',\n  'of',\n  'four',\n  'americans',\n  'believe',\n  'in',\n  'the',\n  'existence',\n  'of',\n  'paranormal',\n  'phenomena',\n  '.'],\n ['tom',\n  'came',\n  'to',\n  'the',\n  'conclusion',\n  'that',\n  'no',\n  'matter',\n  'what',\n  'he',\n  'did',\n  ',',\n  'mary',\n  'wouldn',\n  \"'\",\n  't',\n  'like',\n  'it',\n  '.'],\n ['eighty',\n  'percent',\n  'of',\n  'all',\n  'information',\n  'on',\n  'computers',\n  'around',\n  'the',\n  'world',\n  'is',\n  'in',\n  'english',\n  '.'],\n ['i',\n  'thought',\n  'that',\n  'we',\n  'had',\n  'found',\n  'the',\n  'perfect',\n  'hiding',\n  'place',\n  ',',\n  'but',\n  'the',\n  'police',\n  'found',\n  'us',\n  '.'],\n ['i',\n  \"'\",\n  've',\n  'had',\n  'a',\n  'scratchy',\n  'throat',\n  'since',\n  'this',\n  'morning',\n  '.',\n  'i',\n  'wonder',\n  'if',\n  'i',\n  \"'\",\n  've',\n  'caught',\n  'a',\n  'cold',\n  '.'],\n ['if',\n  'it',\n  'looks',\n  'like',\n  'an',\n  'apple',\n  'and',\n  'it',\n  'tastes',\n  'like',\n  'an',\n  'apple',\n  ',',\n  'it',\n  \"'\",\n  's',\n  'probably',\n  'an',\n  'apple',\n  '.'],\n ['the',\n  'world',\n  'is',\n  'just',\n  'like',\n  'a',\n  'book',\n  ',',\n  'and',\n  'every',\n  'step',\n  'you',\n  'take',\n  'is',\n  'like',\n  'turning',\n  'a',\n  'page',\n  '.'],\n ['today',\n  ',',\n  'i',\n  'was',\n  'supposed',\n  'to',\n  'study',\n  'at',\n  'the',\n  'library',\n  'but',\n  'i',\n  'woke',\n  'up',\n  'around',\n  '12',\n  'o',\n  \"'\",\n  'clock',\n  '.'],\n ['tom',\n  'can',\n  'write',\n  'almost',\n  'like',\n  'a',\n  'native',\n  'speaker',\n  ',',\n  'but',\n  'his',\n  'pronunciation',\n  'is',\n  'terrible',\n  '.'],\n ['tom',\n  'did',\n  'the',\n  'best',\n  'he',\n  'could',\n  ',',\n  'but',\n  'he',\n  'wasn',\n  \"'\",\n  't',\n  'able',\n  'to',\n  'get',\n  'a',\n  'higher',\n  'grade',\n  'than',\n  'mary',\n  '.'],\n ['as',\n  'the',\n  'train',\n  'came',\n  'to',\n  'a',\n  'halt',\n  ',',\n  'all',\n  'of',\n  'the',\n  'passengers',\n  'wondered',\n  'what',\n  'was',\n  'happening',\n  '.'],\n ['charles',\n  'lindbergh',\n  'made',\n  'the',\n  'first',\n  'solo',\n  'flight',\n  'across',\n  'the',\n  'atlantic',\n  'ocean',\n  'in',\n  '1927',\n  '.'],\n ['his',\n  'scores',\n  'are',\n  'always',\n  'better',\n  'than',\n  'mine',\n  ',',\n  'even',\n  'though',\n  'he',\n  'doesn',\n  \"'\",\n  't',\n  'study',\n  'very',\n  'much',\n  '.'],\n ['i',\n  'don',\n  \"'\",\n  't',\n  'have',\n  'a',\n  'lot',\n  'of',\n  'work',\n  ',',\n  'but',\n  'it',\n  \"'\",\n  's',\n  'enough',\n  'to',\n  'keep',\n  'me',\n  'in',\n  'the',\n  'office',\n  'this',\n  'week',\n  '.'],\n ['i',\n  'returned',\n  'the',\n  'books',\n  'i',\n  'borrowed',\n  'from',\n  'the',\n  'library',\n  ',',\n  'and',\n  'i',\n  'borrowed',\n  'some',\n  'new',\n  'ones',\n  '.'],\n ['publication',\n  'of',\n  'the',\n  'article',\n  'was',\n  'timed',\n  'to',\n  'coincide',\n  'with',\n  'the',\n  'professor',\n  \"'\",\n  's',\n  'birthday',\n  '.'],\n ['she',\n  \"'\",\n  's',\n  'popular',\n  ',',\n  'not',\n  'because',\n  'she',\n  \"'\",\n  's',\n  'beautiful',\n  ',',\n  'but',\n  'because',\n  'she',\n  \"'\",\n  's',\n  'kind',\n  'to',\n  'everyone',\n  '.'],\n ['the',\n  'telephone',\n  'operator',\n  'asked',\n  'the',\n  'caller',\n  'to',\n  'hold',\n  'on',\n  'until',\n  'a',\n  'connection',\n  'was',\n  'made',\n  '.'],\n ['whoever',\n  'said',\n  'money',\n  'can',\n  \"'\",\n  't',\n  'buy',\n  'happiness',\n  'simply',\n  'didn',\n  \"'\",\n  't',\n  'know',\n  'where',\n  'to',\n  'go',\n  'shopping',\n  '.'],\n ['at',\n  'the',\n  'time',\n  'there',\n  'were',\n  'no',\n  'native',\n  'english',\n  'speakers',\n  'teaching',\n  'in',\n  'any',\n  'public',\n  'school',\n  '.'],\n ['rio',\n  'de',\n  'janeiro',\n  'is',\n  'perfectly',\n  'safe',\n  'as',\n  'long',\n  'as',\n  'you',\n  'stay',\n  'out',\n  'of',\n  'the',\n  'dangerous',\n  'areas',\n  '.'],\n ['she',\n  'was',\n  'asked',\n  'to',\n  'convince',\n  'him',\n  'to',\n  'get',\n  'his',\n  'son',\n  'or',\n  'someone',\n  'else',\n  'to',\n  'paint',\n  'the',\n  'house',\n  '.'],\n ['tom',\n  'always',\n  'speaks',\n  'in',\n  'such',\n  'a',\n  'low',\n  'voice',\n  'that',\n  'i',\n  'can',\n  'barely',\n  'understand',\n  'what',\n  'he',\n  'says',\n  '.'],\n ['even',\n  'though',\n  'i',\n  'studied',\n  'english',\n  'for',\n  '6',\n  'years',\n  'in',\n  'school',\n  ',',\n  'i',\n  \"'\",\n  'm',\n  'not',\n  'good',\n  'at',\n  'speaking',\n  'it',\n  '.'],\n ['i',\n  'bought',\n  'a',\n  'second',\n  'badminton',\n  'racket',\n  'for',\n  'myself',\n  ',',\n  'but',\n  'i',\n  'forgot',\n  'to',\n  'buy',\n  'a',\n  'shuttlecock',\n  '.'],\n ['i',\n  'didn',\n  \"'\",\n  't',\n  'know',\n  'the',\n  'city',\n  ',',\n  'and',\n  'what',\n  \"'\",\n  's',\n  'more',\n  ',',\n  'i',\n  'couldn',\n  \"'\",\n  't',\n  'speak',\n  'a',\n  'word',\n  'of',\n  'the',\n  'language',\n  '.'],\n ['the',\n  'ages',\n  'of',\n  'the',\n  'two',\n  'children',\n  'put',\n  'together',\n  'was',\n  'equivalent',\n  'to',\n  'that',\n  'of',\n  'their',\n  'father',\n  '.'],\n ['to',\n  'the',\n  'man',\n  'who',\n  'only',\n  'has',\n  'a',\n  'hammer',\n  'in',\n  'the',\n  'toolkit',\n  ',',\n  'every',\n  'problem',\n  'looks',\n  'like',\n  'a',\n  'nail',\n  '.'],\n ['tom',\n  'went',\n  'swimming',\n  'in',\n  'the',\n  'river',\n  ',',\n  'but',\n  'when',\n  'he',\n  'got',\n  'out',\n  ',',\n  'his',\n  'clothes',\n  'had',\n  'been',\n  'stolen',\n  '.'],\n ['being',\n  'a',\n  'good',\n  'conversationalist',\n  'does',\n  'not',\n  'just',\n  'mean',\n  'being',\n  'a',\n  'good',\n  'speaker',\n  'of',\n  'english',\n  '.'],\n ['manholes',\n  'are',\n  'round',\n  'because',\n  'that',\n  'way',\n  'they',\n  'won',\n  \"'\",\n  't',\n  'accidentally',\n  'fall',\n  'through',\n  'the',\n  'hole',\n  '.']]"},"exec_count":4}},"pos":5,"start":1655233024438,"state":"done","type":"cell"}
{"cell_type":"code","end":1655233024596,"exec_count":5,"id":"173bd5","input":"'''\nYour code here: Explore this data. Can you calculate the maximum length of a sequence in each dataset english and chinese?\n'''\n# calculate max_len of any sequence in 'english' list and save it to a variable called max_english_length\nmax_english_length = max([len(seq) for seq in english])\nprint(max_english_length)\n# calculate max_len of any sequence in 'chinese' list and save it to a variable called max_chinese_length\nmax_chinese_length = max([len(seq) for seq in chinese])\nprint(max_chinese_length)","kernel":"nlp_env","no_halt":true,"output":{"0":{"name":"stdout","text":"25\n39\n"}},"pos":6,"scrolled":true,"start":1655233024588,"state":"done","type":"cell"}
{"cell_type":"code","end":1655233024629,"exec_count":6,"id":"998d2d","input":"input_tokenizer = Word2Sequence()\nfor words in english:\n    input_tokenizer.fit(words)\ninput_tokenizer.build_vocab(min=1, max_features=None) #inpu\n\noutput_tokenizer = Word2Sequence()\nfor words in chinese:\n    output_tokenizer.fit(words)\noutput_tokenizer.build_vocab(min=1, max_features=None)\n\n'''\nYour code here: print the total english words in your input tokenizer and total chinese words in your output tokenizer below!\n'''\ntotal_english_words = len(input_tokenizer)\nprint(total_english_words)\n\ntotal_chinese_words = len(output_tokenizer)\nprint(total_chinese_words)","kernel":"nlp_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzoUqCrq4fWj","outputId":"4290ee15-457e-44d7-f083-f12a2e5d78a2"},"no_halt":true,"output":{"0":{"name":"stdout","text":"199\n317\n"}},"pos":7,"start":1655233024612,"state":"done","type":"cell"}
{"cell_type":"code","end":1655233024653,"exec_count":7,"id":"1c59b4","input":"# Seq2Seq Parameters\nin_maxlen = 25 + 1 # 25 + 1(<EOS> token)\nout_maxlen = 39 + 1 # 39 + 1(<EOS> token or <SOS> token)\nn_hidden = 32 # number of \"neurons\" per layer\nd_model = 64 # number of embedding dimensions to represent each word\nenc_n_class = len(input_tokenizer.dict) # OR... vocab size of englisth -> 199\ndec_n_class = len(output_tokenizer.dict) # OR... vocab size of chinese -> 317\nbatch_size = 1","kernel":"nlp_env","metadata":{"id":"qm_Qhnlk49Rn"},"no_halt":true,"pos":9,"start":1655233024650,"state":"done","type":"cell"}
{"cell_type":"code","end":1655233024676,"exec_count":8,"id":"d89519","input":"# max_english_length, max_chinese_length = 25, 39\neng_maxlen = max_english_length + 1 # 25 + 1(<EOS> token)\nchin_maxlen = max_chinese_length + 1  # 39 + 1(<EOS> token or <SOS> token)\nbatch_size = 1\n\n# Setup the Dataset.\ndataset = Dataset(\n    X = english,\n    Y = chinese,\n    in_tknz = input_tokenizer, out_tknz = output_tokenizer,\n    in_maxlen = eng_maxlen, out_maxlen = chin_maxlen\n)\n\n'''\nThe following are helper functions to help pytorch. You won't need to know this much.\n'''\n# NOTE: collate_fn preprocesses your input from PyTorch Dataset above during PyTorch DataLoader\ndef collate_fn(batch):\n    '''\n    param batch: ([enc_in, dec_in, dec_out]， [enc_in, dec_in, dec_out], output of getitem...)\n    '''\n    # unpack values\n    enc_in, dec_in, dec_out = list(zip(*batch))\n    # Return tensor type\n    return torch.LongTensor(enc_in), torch.LongTensor(dec_in), torch.LongTensor(dec_out)\n\ndef get_dataloader(dataset, batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn):\n    '''\n    Returns a way to access and use the data\n    '''\n    dataloader = DataLoader(dataset=dataset,\n                            batch_size=batch_size,\n                            shuffle=shuffle,\n                            drop_last=drop_last,\n                            collate_fn=collate_fn)\n    return dataloader\n# Get PyTorch DataLoader\ndataloader = get_dataloader(dataset, batch_size)\ndataloader = get_dataloader(dataset, batch_size)","kernel":"nlp_env","no_halt":true,"pos":10,"start":1655233024664,"state":"done","type":"cell"}
{"cell_type":"code","end":1655233024719,"exec_count":9,"id":"2dbc0f","input":"model = Seq2Seq(\n    in_maxlen = in_maxlen,\n    out_maxlen = out_maxlen,\n    n_hidden = n_hidden,\n    enc_n_class = len(input_tokenizer.dict),\n    dec_n_class = len(output_tokenizer.dict),\n    d_model = d_model,\n    num_layers = 1,\n)\nmodel.to(device)\n# # If you have saved a model before\n# model.load_state_dict(torch.load(\"seq2seq.pt\", map_location=device))","kernel":"nlp_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVVbVP7l5AJg","outputId":"e28da9fa-5fa2-4fbd-b4d3-49de9999475b"},"no_halt":true,"output":{"0":{"name":"stderr","text":"/projects/0ddeade5-3577-4fe8-8cd6-8a0cb653428e/miniconda3/envs/nlp_env/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n"},"1":{"data":{"text/plain":"Seq2Seq(\n  (encoder): GRU(64, 32, dropout=0.3)\n  (decoder): GRU(64, 32, dropout=0.3)\n  (embed_enc): Embedding(199, 64)\n  (embed_dec): Embedding(317, 64)\n  (fc): Linear(in_features=32, out_features=317, bias=True)\n)"},"exec_count":9}},"pos":11,"start":1655233024686,"state":"done","type":"cell"}
{"cell_type":"code","end":1655233024745,"exec_count":10,"id":"4c715e","input":"# Define Loss and Optimizer -- these are ways we define performance for our model. If you're curious: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\ncriterion = nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=1e-2)","kernel":"nlp_env","metadata":{"id":"PtNvjmBo5A8W"},"no_halt":true,"pos":12,"start":1655233024741,"state":"done","type":"cell"}
{"cell_type":"code","end":1655234277896,"exec_count":11,"id":"ed4e88","input":"'''\nYour code here: change the number of epochs to see how it effects training time and quality\n'''\nepochs = 500\n\n\n'''\nTraining -- no need to touch the code below.\n'''\ntorch.cuda.empty_cache()\nmodel.train()\nmodel.to(device)\nloss_records = []\n\n\nfor epoch in range(epochs):\n    # runs the model and calculates loss\n    loss = 0\n    for _, (enc_in, dec_in, dec_out) in enumerate(dataloader):\n        # enc_h_0.shape: [1(num_layers), 1(batch_size), 32(hidden_size)]\n        enc_h_0 = model.init_enc_hidden_GRU(batch_size, device)\n        # To Cuda Device if available\n        enc_in, dec_in = enc_in.to(device), dec_in.to(device)\n        \n        pred = model(enc_in, enc_h_0, dec_in)\n        \n        dec_out = dec_out.to(device)\n        for i in range(len(dec_out)): # dec_in.shape: [1(b), 40(out_maxlen)]\n            # pred[i].shape: [40(out_maxlen), 317(dec_n_class)]\n            # dec_out[i].shape: [40(out_maxlen)]\n            loss += criterion(pred[i], dec_out[i])\n\n    if (epoch) % 10 == 0:\n        print(f\"Epoch: {epoch}, Loss: {loss}\")\n\n    if (epoch) % 100 == 0:\n        loss_records.append(loss)\n    \n    # runs the actual back propacation\n    optim.zero_grad()\n    loss.backward()\n    optim.step()\n    torch.save(model.state_dict(), \"seq2seq.pt\")","kernel":"nlp_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PTSlF8fk5QpG","outputId":"3eb0df28-80b9-452b-e9b5-5c7ffff27c98"},"no_halt":true,"output":{"0":{"name":"stdout","text":"Epoch: 0, Loss: 576.2747802734375\n"},"1":{"name":"stdout","text":"Epoch: 10, Loss: 280.32415771484375\n"},"10":{"name":"stdout","text":"Epoch: 100, Loss: 72.41958618164062\n"},"11":{"name":"stdout","text":"Epoch: 110, Loss: 59.867557525634766\n"},"12":{"name":"stdout","text":"Epoch: 120, Loss: 49.38654708862305\n"},"13":{"name":"stdout","text":"Epoch: 130, Loss: 40.73469161987305\n"},"14":{"name":"stdout","text":"Epoch: 140, Loss: 33.71730041503906\n"},"15":{"name":"stdout","text":"Epoch: 150, Loss: 28.130598068237305\n"},"16":{"name":"stdout","text":"Epoch: 160, Loss: 23.55632781982422\n"},"17":{"name":"stdout","text":"Epoch: 170, Loss: 19.924890518188477\n"},"18":{"name":"stdout","text":"Epoch: 180, Loss: 16.938188552856445\n"},"19":{"name":"stdout","text":"Epoch: 190, Loss: 14.635987281799316\n"},"2":{"name":"stdout","text":"Epoch: 20, Loss: 238.6700897216797\n"},"20":{"name":"stdout","text":"Epoch: 200, Loss: 12.67483139038086\n"},"21":{"name":"stdout","text":"Epoch: 210, Loss: 11.050590515136719\n"},"22":{"name":"stdout","text":"Epoch: 220, Loss: 9.96059513092041\n"},"23":{"name":"stdout","text":"Epoch: 230, Loss: 8.723247528076172\n"},"24":{"name":"stdout","text":"Epoch: 240, Loss: 7.7913641929626465\n"},"25":{"name":"stdout","text":"Epoch: 250, Loss: 7.6860761642456055\n"},"26":{"name":"stdout","text":"Epoch: 260, Loss: 6.983063220977783\n"},"27":{"name":"stdout","text":"Epoch: 270, Loss: 6.475579738616943\n"},"28":{"name":"stdout","text":"Epoch: 280, Loss: 5.597828388214111\n"},"29":{"name":"stdout","text":"Epoch: 290, Loss: 5.036618709564209\n"},"3":{"name":"stdout","text":"Epoch: 30, Loss: 217.3397674560547\n"},"30":{"name":"stdout","text":"Epoch: 300, Loss: 4.595510482788086\n"},"31":{"name":"stdout","text":"Epoch: 310, Loss: 4.232182502746582\n"},"32":{"name":"stdout","text":"Epoch: 320, Loss: 3.958742618560791\n"},"33":{"name":"stdout","text":"Epoch: 330, Loss: 3.670031785964966\n"},"34":{"name":"stdout","text":"Epoch: 340, Loss: 3.41642165184021\n"},"35":{"name":"stdout","text":"Epoch: 350, Loss: 3.19545316696167\n"},"36":{"name":"stdout","text":"Epoch: 360, Loss: 2.9968225955963135\n"},"37":{"name":"stdout","text":"Epoch: 370, Loss: 2.8598716259002686\n"},"38":{"name":"stdout","text":"Epoch: 380, Loss: 2.6651551723480225\n"},"39":{"name":"stdout","text":"Epoch: 390, Loss: 2.5117602348327637\n"},"4":{"name":"stdout","text":"Epoch: 40, Loss: 193.51229858398438\n"},"40":{"name":"stdout","text":"Epoch: 400, Loss: 2.38970947265625\n"},"41":{"name":"stdout","text":"Epoch: 410, Loss: 2.2810122966766357\n"},"42":{"name":"stdout","text":"Epoch: 420, Loss: 2.163372039794922\n"},"43":{"name":"stdout","text":"Epoch: 430, Loss: 2.044552803039551\n"},"44":{"name":"stdout","text":"Epoch: 440, Loss: 1.9543917179107666\n"},"45":{"name":"stdout","text":"Epoch: 450, Loss: 1.8725786209106445\n"},"46":{"name":"stdout","text":"Epoch: 460, Loss: 1.7766356468200684\n"},"47":{"name":"stdout","text":"Epoch: 470, Loss: 1.69122314453125\n"},"48":{"name":"stdout","text":"Epoch: 480, Loss: 1.9904934167861938\n"},"49":{"name":"stdout","text":"Epoch: 490, Loss: 9.795100212097168\n"},"5":{"name":"stdout","text":"Epoch: 50, Loss: 168.69577026367188\n"},"6":{"name":"stdout","text":"Epoch: 60, Loss: 144.92808532714844\n"},"7":{"name":"stdout","text":"Epoch: 70, Loss: 123.36004638671875\n"},"8":{"name":"stdout","text":"Epoch: 80, Loss: 104.08132934570312\n"},"9":{"name":"stdout","text":"Epoch: 90, Loss: 87.08438873291016\n"}},"pos":14,"start":1655233024758,"state":"done","type":"cell"}
{"cell_type":"code","end":1655234279173,"exec_count":12,"id":"aa9b77","input":"import matplotlib.pyplot as plt\nplt.switch_backend('agg')\n%matplotlib inline\nimport matplotlib.ticker as ticker\nimport numpy as np\n\ndef showPlot(points): # Helper function for showing our plots\n    plt.figure()\n    fig, ax = plt.subplots()\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)","kernel":"nlp_env","metadata":{"id":"DCWlner5CkY-"},"no_halt":true,"pos":16,"start":1655234277933,"state":"done","type":"cell"}
{"cell_type":"code","end":1655234315196,"exec_count":13,"id":"89a7df","input":"showPlot([loss.cpu().item() for loss in loss_records])","kernel":"nlp_env","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":720},"id":"WGjbNIopCkzd","outputId":"dc2375cb-efe2-4756-dd01-309e4f056e1a"},"no_halt":true,"output":{"0":{"data":{"text/plain":"<Figure size 432x288 with 0 Axes>"}},"1":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"10":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"11":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"12":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"13":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"14":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"15":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"16":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"17":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"18":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"19":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"2":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"20":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"21":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"22":{"data":{"image/png":"26a76167cff571fc1bef232a1f5cae2ecb8bc49d","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}},"3":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"4":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"5":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"6":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"7":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"8":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"},"9":{"name":"stderr","text":"Locator attempting to generate 3158 ticks ([-26.400000000000002, ..., 605.0000000000001]), which exceeds Locator.MAXTICKS (1000).\n"}},"pos":17,"start":1655234279205,"state":"done","type":"cell"}
{"cell_type":"code","end":1655237198977,"exec_count":18,"id":"7e5fe1","input":"'''\nNo need to touch this code: \n'''\n\ndef translate(eng_sent, model, device):\n    # set up the inputs and variables\n    model.eval()\n    model.to(device)\n    eng_sent = tk.tokenize(eng_sent.lower()) + [\"<EOS>\"]\n    eng_sent = input_tokenizer.transform(eng_sent, max_len=in_maxlen, pad_first=False)\n    dec_in = ([\"<SOS>\"] + [\"<PAD>\"]*out_maxlen)[:out_maxlen]\n    dec_in = output_tokenizer.transform(dec_in, max_len=out_maxlen, pad_first=False)\n    \n    enc_h_0 = model.init_enc_hidden_GRU(batch_size, device)\n    eng_sent, dec_in = torch.LongTensor(eng_sent), torch.LongTensor(dec_in)\n\n    eng_sent = eng_sent.unsqueeze(0)\n    dec_in = dec_in.unsqueeze(0)\n    eng_sent, dec_in = eng_sent.to(device), dec_in.to(device)\n\n    # run the model\n    with torch.no_grad():\n        # eng_sent: [1(b), 26(in_maxlen)]\n        embedded_X = model.embed_enc(eng_sent)\n        # embedded_X: [26(in_maxlen), 1(b), 64(d_model)] <- [1(b), 26(in_maxlen), 64(d_model)]\n        embedded_X = embedded_X.permute(1, 0, 2)\n        _, memory = model.encoder(embedded_X, enc_h_0)\n        pred_loc = 0\n        for i in range(out_maxlen-1):\n            embedded_Y = model.embed_dec(dec_in)\n            embedded_Y = embedded_Y.permute(1, 0, 2)\n            outputs, _ = model.decoder(embedded_Y, memory)\n            outputs = outputs.permute(1, 0, 2)\n            pred = model.fc(outputs)\n            pred = pred[0][pred_loc].topk(1)[1].item()\n            pred_loc += 1\n            if pred == 2:\n                dec_in[0][pred_loc] = pred\n                break\n            else:\n                dec_in[0][pred_loc] = pred\n    return dec_in","kernel":"nlp_env","metadata":{"id":"XHs9IRJK8usV"},"pos":19,"start":1655237198966,"state":"done","type":"cell"}
{"cell_type":"code","end":1655237203920,"exec_count":19,"id":"1d6f6e","input":"import random\neng_sents = random.sample(input_texts, 5)\nfor sent in eng_sents:\n  translated = translate(sent, model, torch.device(\"cpu\"))\n  translated_sent = output_tokenizer.inverse_transform(translated[0], is_tensor=True)\n  translated_sent = \"\".join([word for word in translated_sent if word != \"<SOS>\" and word != \"<EOS>\"and word != \"<PAD>\"])\n  print(f\"{sent} -> \\n{translated_sent}\")","kernel":"nlp_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fh0-iZdvBUFF","outputId":"f42cb9ed-a961-4d0f-c94c-6be4fb8d6656"},"output":{"0":{"name":"stdout","text":"The importation of rare wild animals to this country is strictly prohibited. -> \n该国<UNK><UNK><UNK>口<UNK>有野生<UNK>物。\nCharles Lindbergh made the first solo flight across the Atlantic Ocean in 1927. -> \n<UNK>har<UNK>e<UNK> <UNK><UNK><UNK><UNK><UNK>er<UNK>h於192<UNK>年成<UNK>完成了<UNK>一次<UNK>自飛<UNK>大西<UNK>。\nWe were talking about something at that time, but I don't remember what. -> \n我们那时在<UNK>论事情，但我不记得是什么了。\nI'm a foreigner and I don't know Czech very well. Please, speak slowly. -> \n我是外国人，<UNK>的<UNK>语不好，<UNK>说<UNK>一点。\nIf it's at all possible, I'd like you to take part in the next meeting. -> \n如果你想去，就去好了。如果你不想去，那也没什么大不了的。\n"}},"pos":21,"start":1655237203652,"state":"done","type":"cell"}
{"cell_type":"code","end":1655237206992,"exec_count":20,"id":"8b9ae6","input":"'''\nYour code here: translate custom sentences using the code above. Hint: You won't need a for loop!\n'''\neng_sents = ['I am a student.', 'What']\nfor sent in eng_sents:\n  translated = translate(sent, model, torch.device(\"cpu\"))\n  translated_sent = output_tokenizer.inverse_transform(translated[0], is_tensor=True)\n  translated_sent = \"\".join([word for word in translated_sent if word != \"<SOS>\" and word != \"<EOS>\"and word != \"<PAD>\"])\n  print(f\"{sent} -> \\n{translated_sent}\")","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"I am a student. -> \n我想知道这个城市，而且我一点都不懂那里的语言。\nWhat -> \n我想知道最<UNK>的美國<UNK>通<UNK>事<UNK>的電話<UNK><UNK>。\n"}},"pos":23,"scrolled":true,"start":1655237206806,"state":"done","type":"cell"}
{"cell_type":"code","id":"374595","input":"","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"1b8f51","input":"# The Task at Hand\n\nHave you ever wondered if computers could translate languages? Did you think google translate or duolingo worked because they memorized answers? \n\nThe type of problem that translation solves is sequence to sequence. For instance, we could convert an english input sequence to a german output sequence. \n\nIn this activity, we will create an english -> chinese translator and apply what we've been learning about LSTMs & RNNs.\n\n","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"318e4c","input":"<h1>Training our model</h1>","metadata":{"id":"wVpmLnAtBLEV"},"pos":13,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"837032","input":"<h1>Creating the model</h1>\n\n![image.png](data:image/png;base64,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)\n\nNOTE: For Structure of Encoder Inputs, they can all be either (assume all have same maxlen): \n1. \\<SOS>, word1, word2, word3, ..., \\<EOS>\n2. word1, word2, word3, ..., \\<EOS> \n3. word1, word2, word3, ...\n\nNOTE: But Decoder In and Out structures should always look like this (assume all have same maxlen):\n- Decoder Input: \\<SOS>, word1, word2, word3, ...\n- Decoder Output: word1, word2, word3, ..., \\<EOS>  \n\nThis means that our input and ouput max length should be one more than the sequence's max length.\n\nWHY? Data Structure:\n- 1. Encoder Input: [word1, word2, ... + <EOS>]\n- 2. Decoder Input: [<SOS> + word1, word2, ...]\n- 3. Decoder Output:[word1, word2, ... + <EOS>]\n    \nnn docs - https://pytorch.org/docs/stable/nn.html","pos":8,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"843aac","input":"<h1>Code for Translating with our Model</h1>\nThis is where the Seq2Seq happens after the model is trained.\n","pos":18,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a3aadb","input":"# Using our Model in Practice\nCheck out these examples below. This is how you can translate sequences!","metadata":{"id":"kG_fTQzoBsMu"},"pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"bbd834","input":"# Your turn!\n\nCan you use the code in the cell above to translate custom sentences? \n\n","pos":22,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"dcf4c2","input":"## Let's check out our model's progress\nNo need to change the code below, this will plot our loss over time. How do you think we can tweak our code to decrease loss even further?","pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"eb6065","input":"## Preparing the data\nWe need to import our packages and data to learn a little bit about the problem at hand.","pos":1,"state":"done","type":"cell"}
{"id":0,"time":1655251447659,"type":"user"}
{"last_load":1655230393337,"type":"file"}