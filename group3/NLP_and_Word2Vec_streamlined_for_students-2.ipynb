{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Natural Language Processing using NLTK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /projects/0ddeade5-3577-4fe\n",
      "[nltk_data]     8-8cd6-8a0cb653428e/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /projects/0ddeade5-3577-4fe8-\n",
      "[nltk_data]     8cd6-8a0cb653428e/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install NLTK - pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 0 - Get some Data!\n",
    "\n",
    "This section's code is mostly given to you as a review for how you can scrape and manipulate data from the web. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import urllib\n",
    "import bs4 as bs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# We will read the contents of the Wikipedia article \"Global_warming\" as an example, please feel free to use your own! You can use the url below:\n",
    "url = 'https://en.wikipedia.org/wiki/Global_warming' # you can change this to use other sites as well.\n",
    "\n",
    "# We can open the page using \"urllib.request.urlopen\" then read it using \".read()\"\n",
    "source = urllib.request.urlopen(url).read()\n",
    "\n",
    "# Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n",
    "# you may need to install a parser library --> \"!pip3 install lxml\"\n",
    "# Parsing the data/creating BeautifulSoup object\n",
    "\n",
    "soup = bs.BeautifulSoup(source,\"html.parser\") \n",
    "\n",
    "# Fetching the data\n",
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n",
    "    text += paragraph.text\n",
    "\n",
    "# Preprocessing the data\n",
    "\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\n",
    "text = text.lower() #everything to lowercase\n",
    "text = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\n",
    "text = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\n",
    "text = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" contemporary climate change includes both global warming and its impacts on earth's weather patterns. there have been previous periods of climate change, but the current changes are distinctly more rapid and not due to natural causes. instead, they are caused by the emission of greenhouse gases, mostly carbon dioxide (co ) and methane. burning fossil fuels for energy use creates most of these emi\""
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 1 - Tokenization of paragraphs/sentences\n",
    "\n",
    "In this section we are going to tokenize our sentences and words. If you aren't familiar with tokenization, we recommend looking up \"what is tokenization\". \n",
    "\n",
    "You should also spend time on the [NLTK documentation](https://www.nltk.org/). If you're not sure how to do something, or get an error, it is best to google it first and ask questions as you go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contemporary',\n",
       " 'climate',\n",
       " 'change',\n",
       " 'includes',\n",
       " 'both',\n",
       " 'global',\n",
       " 'warming',\n",
       " 'and',\n",
       " 'its',\n",
       " 'impacts',\n",
       " 'on',\n",
       " 'earth',\n",
       " \"'s\",\n",
       " 'weather',\n",
       " 'pattern']"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'''\n",
    "#Your code here: Tokenize the words from the data and set it to a variable called words.\n",
    "#Hint: how to this might be on the very home page of NLTK!\n",
    "#'''\n",
    "import nltk\n",
    "sentence = \"contemporary climate change includes both global warming and its impacts on earth's weather pattern\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contemporary', 'climate', 'change', 'includes', 'both', 'global', 'warming', 'and', 'its', 'impacts']\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" contemporary climate change includes both global warming and its impacts on earth's weather patterns\",\n",
       " ' there have been previous periods of climate change, but the current changes are distinctly more rapid and not due to natural causes',\n",
       " ' instead, they are caused by the emission of greenhouse gases, mostly carbon dioxide (co ) and methane',\n",
       " ' burning fossil fuels for energy use creates most of these emi']"
      ]
     },
     "execution_count": 35,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'''\n",
    "#Your code here: Tokenize the sentences from the data  and set it to a variable called sentences.\n",
    "#Hint: try googling how to tokenize sentences in NLTK!\n",
    "#'''\n",
    "\n",
    "import nltk\n",
    "text = \" contemporary climate change includes both global warming and its impacts on earth's weather patterns. there have been previous periods of climate change, but the current changes are distinctly more rapid and not due to natural causes. instead, they are caused by the emission of greenhouse gases, mostly carbon dioxide (co ) and methane. burning fossil fuels for energy use creates most of these emi\"\n",
    "sentences = text.split('.')\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" contemporary climate change includes both global warming and its impacts on earth's weather patterns\", ' there have been previous periods of climate change, but the current changes are distinctly more rapid and not due to natural causes', ' instead, they are caused by the emission of greenhouse gases, mostly carbon dioxide (co ) and methane', ' burning fossil fuels for energy use creates most of these emi']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 2 - Stopwords and Punctuation\n",
    "Now we are going to work to remove stopwords and punctuation from our data. Why do you think we are going to do this? Do some research if you don't know yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /projects/0ddeade5-3577-4\n",
      "[nltk_data]     fe8-8cd6-8a0cb653428e/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"contemporary climate change includes global warming impacts earth 's weather patterns\", 'previous periods climate change , current changes distinctly rapid due natural causes', 'instead , caused emission greenhouse gases , mostly carbon dioxide ( co ) methane', 'burning fossil fuels energy use creates emi']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rem_stopwords(sentence):\n",
    "    return ' '.join([word for word in nltk.word_tokenize(sentence) if word not in stopwords.words('english')])\n",
    "\n",
    "sentences = [rem_stopwords(sentence) for sentence in sentences]\n",
    "print(sentences[:100]) #Check if it worked correctly. Are all stopwords removed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contemporary', 'climate', 'change', 'includes']\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "\n",
    "#define a function called \"remove_punctuation\" that removes punctuation from the sentences.\n",
    "#'''\n",
    "import re\n",
    "import string\n",
    "def convert(string):\n",
    "    list1=[]\n",
    "    list1[:0]=string\n",
    "    return list1\n",
    "punctlist = convert(string.punctuation)\n",
    "\n",
    "def remove_punctuation(sentences):\n",
    "    sentences_list = []\n",
    "#    ### Some code goes here. Hint: Try looking up how to remove stopwords in NLTK if you get stuck. ###\n",
    "    for i in range(4):\n",
    "        sentences_without_punctuation = re.sub(r'[^\\w\\s]','', sentences[i])\n",
    "        sentences_list.append(sentences_without_punctuation)\n",
    "    return sentences_list\n",
    "'''\n",
    "def rem_punct(sentence):\n",
    "    return \" \".join([word for word in nltk.word_tokenize(sentence) if word not in punctlist])\n",
    "'''\n",
    "sentences = remove_punctuation(sentences)\n",
    "print(sentences[:100]) #eliminating all punctuation.\n",
    "\n",
    "#i think it's because sentences is a list and not a string\n",
    "#yeah you're right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 3a - Stemming the words\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. There is an example below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troubl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# try each of the words below\n",
    "x=stemmer.stem('troubled')\n",
    "#stemmer.stem('trouble')\n",
    "#stemmer.stem('troubling')\n",
    "#stemmer.stem('troubles')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contemporari climat chang includ global warm impact earth s weather pattern previou period climat chang current chang distinctli rapid due natur caus instead caus emiss greenhous gase mostli carbon dioxid co methan burn fossil fuel energi use creat emi "
     ]
    }
   ],
   "source": [
    "#'''\n",
    "#Your code here:\n",
    "#Define a function called \"stem_sentences\" that takes in a list of sentences and returns a list of stemmed sentences.\n",
    "#'''\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "def stem_sentences(sentences):\n",
    "    ### Some code goes here. Hint: Try looking up how to stem words in NLTK if you get stuck (or simply use the example above and run stemmer in a loop!). ###\n",
    "    stemmed = []\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        for x in range(len(words)):\n",
    "            stemmed.append(stemmer.stem(words[x]))\n",
    "    for j in range(len(stemmed)):\n",
    "        print(stemmed[j], end = \" \" )\n",
    "    return stemmed\n",
    "sentences = stem_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contemporari', 'climat', 'chang', 'includ', 'global', 'warm', 'impact', 'earth', 's', 'weather', 'pattern', 'previou', 'period', 'climat', 'chang', 'current', 'chang', 'distinctli', 'rapid', 'due', 'natur', 'caus', 'instead', 'caus', 'emiss', 'greenhous', 'gase', 'mostli', 'carbon', 'dioxid', 'co', 'methan', 'burn', 'fossil', 'fuel', 'energi', 'use', 'creat', 'emi']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 3b - Lemmatization\n",
    "Lemmatization considers the context and converts the word to its meaningful base form. There is a cool tutorial and definition of lemmatization in NLTK [here](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contemporary climate change includes both global warming and it impact on earth 's weather pattern there have been previous period of climate change , but the current change are distinctly more rapid and not due to natural cause instead , they are caused by the emission of greenhouse gas , mostly carbon dioxide ( co ) and methane burning fossil fuel for energy use creates most of these emi "
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "## Step 1: Import the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "'''\n",
    "Your code here: Define a function called \"lem_sentences\" that: loops through the sentences, split the sentences up by words and applies \"lemmatizer.lemmatize\" to each word and then join everything back into a sentence\n",
    "'''\n",
    "lem_list = []\n",
    "##Similar to stopwords: For loop through the sentences, split by words and apply \"lemmatizer.lemmatize\" to each word and join back into a sentence\n",
    "def lem_sentences(sentences):\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        for x in range(len(words)): \n",
    "            lem_list.append(lemmatizer.lemmatize(words[x]))\n",
    "    for j in range(len(lem_list)):\n",
    "        print(lem_list[j], end = \" \")\n",
    "    return lem_list\n",
    "sentences = lem_sentences(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contemporary', 'climate', 'change', 'includes', 'both', 'global', 'warming', 'and', 'it', 'impact', 'on', 'earth', \"'s\", 'weather', 'pattern', 'there', 'have', 'been', 'previous', 'period', 'of', 'climate', 'change', ',', 'but', 'the', 'current', 'change', 'are', 'distinctly', 'more', 'rapid', 'and', 'not', 'due', 'to', 'natural', 'cause', 'instead', ',', 'they', 'are', 'caused', 'by', 'the', 'emission', 'of', 'greenhouse', 'gas', ',', 'mostly', 'carbon', 'dioxide', '(', 'co', ')', 'and', 'methane', 'burning', 'fossil', 'fuel', 'for', 'energy', 'use', 'creates', 'most', 'of', 'these', 'emi']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 4 - POS Tagging\n",
    "Parts of speech tagging is marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to /project\n",
      "[nltk_data]     s/0ddeade5-3577-4fe8-8cd6-8a0cb653428e/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contemporary_JJ climate_NN change_NN includes_VBZ both_DT global_JJ warming_NN and_CC its_PRP$ impacts_NNS on_IN earth_NN 's_POS weather_NN patterns_NNS ._. there_EX have_VBP been_VBN previous_JJ periods_NNS of_IN climate_NN change_NN ,_, but_CC the_DT current_JJ changes_NNS are_VBP distinctly_RB more_RBR rapid_JJ and_CC not_RB due_JJ to_TO natural_JJ causes_NNS ._. instead_RB ,_, they_PRP are_VBP caused_VBN by_IN the_DT emission_NN of_IN greenhouse_NN gases_NNS ,_, mostly_RB carbon_NN dioxide_NN (_( co_NN )_) and_CC methane_NN ._. burning_VBG fossil_JJ fuels_NNS for_IN energy_NN use_NN creates_VBZ most_JJS of_IN these_DT emi_NNS\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging example\n",
    "# CC - coordinating conjunction\n",
    "# NN - noun, singular (cat, tree)\n",
    "all_words = nltk.word_tokenize(text)  ###If we want to look at part of speech taking before we stem/lem\n",
    "\n",
    "tagged_words = nltk.pos_tag(all_words)\n",
    "##Creates a list of lists where each element of the list is [word,partofspeech abbreviation]\n",
    "\n",
    "# Tagged word paragraph\n",
    "word_tags = []\n",
    "for tw in tagged_words:\n",
    "    word_tags.append(tw[0]+\"_\"+tw[1])\n",
    "\n",
    "tagged_paragraph = ' '.join(word_tags)\n",
    "\n",
    "'''\n",
    "Your code here: print the first 1000 characters of tagged_paragraph.\n",
    "'''\n",
    "print(tagged_paragraph[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Word2Vec Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /projects/0ddeade5-3577-4fe8-\n",
      "[nltk_data]     8cd6-8a0cb653428e/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install gensim - pip install gensim\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "#Let's go ahead and create a list that's formatted how word2vec needs:\n",
    "    # a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence (after preprocessing)\n",
    "'''\n",
    "url2 = 'https://en.wikipedia.org/wiki/Global_warming'\n",
    "source2 = urllib.request.urlopen(url2).read()\n",
    "soup2 = bs.BeautifulSoup(source2,\"html.parser\")\n",
    "text2 = \"\"\n",
    "for paragraph in soup2.find_all('p'):\n",
    "    text2 += paragraph.text\n",
    "text2 = re.sub(r'\\[[0-9]*\\]',' ',text2)\n",
    "text2 = text.lower()\n",
    "text2 = re.sub(r'\\W^.?!',' ',text2)\n",
    "text2 = re.sub(r'\\d',' ',text2)\n",
    "text2 = re.sub(r'\\s+',' ',text2)\n",
    "'''\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "sentences2 = nltk.sent_tokenize(text)\n",
    "sentences2 = [rem_stopwords(sentence) for sentence in sentences2]\n",
    "sentences2 = remove_punctuation(sentences2)\n",
    "word_list = [nltk.word_tokenize(sentence) for sentence in sentences2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['contemporary', 'climate', 'change', 'includes', 'global', 'warming', 'impacts', 'earth', 's', 'weather', 'patterns'], ['previous', 'periods', 'climate', 'change', 'current', 'changes', 'distinctly', 'rapid', 'due', 'natural', 'causes'], ['instead', 'caused', 'emission', 'greenhouse', 'gases', 'mostly', 'carbon', 'dioxide', 'co', 'methane'], ['burning', 'fossil', 'fuels', 'energy', 'use', 'creates', 'emi']]\n"
     ]
    }
   ],
   "source": [
    "# print the tokenized list of lists\n",
    "print(word_list [:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "For this part you may want to follow a guide [here](https://radimrehurek.com/gensim/models/word2vec.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "''' Training the Word2Vec model. You should pass:\n",
    "1. a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence\n",
    "2. min_count=1 --> Ignores all words with total frequency lower than 1 (i.e., include everything).\n",
    "'''\n",
    "# create the model\n",
    "model = Word2Vec(word_list , min_count=1)\n",
    "\n",
    "# get the most common words of the model (it's entire vocabulary)\n",
    "most_common_words = model.wv.index_to_key\n",
    "\n",
    "# save the model to use it later\n",
    "model.save(\"word2vec.model\")\n",
    "# model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['climate', 'change', 'emi', 'weather', 'distinctly', 'changes', 'current', 'periods', 'previous', 'patterns']\n"
     ]
    }
   ],
   "source": [
    "#print the first 10 most common words.\n",
    "print(most_common_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2Vec' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Look up the most similar words to certain words in your text using the model.wv.most_similar() function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/gensim/models/keyedvectors.py:758\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    755\u001b[0m     clip_end \u001b[38;5;241m=\u001b[39m restrict_vocab\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# add weights for each key, if not already present; default to 1.0 for positive and -1.0 for negative keys\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m positive \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    759\u001b[0m     (item, \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _EXTENDED_KEY_TYPES) \u001b[38;5;28;01melse\u001b[39;00m item\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m positive\n\u001b[1;32m    761\u001b[0m ]\n\u001b[1;32m    762\u001b[0m negative \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    763\u001b[0m     (item, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _EXTENDED_KEY_TYPES) \u001b[38;5;28;01melse\u001b[39;00m item\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m negative\n\u001b[1;32m    765\u001b[0m ]\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Word2Vec' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Look up the most similar words to certain words in your text using the model.wv.most_similar() function\n",
    "most_similar_words = model.wv.most_similar\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing our model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    # Finding Word Vectors - print word vectors for certain words in your text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    ### Finding the most similar words in the model ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "similar1, similar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# code to print a wordcloud for your sentences\n",
    "wordcloud = WordCloud(\n",
    "                        background_color='white',\n",
    "                        max_words=100,\n",
    "                        max_font_size=50, \n",
    "                        random_state=42\n",
    "                        ).generate(str(sentences))\n",
    "fig = plt.figure(1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Why did we do all this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# reFetching the data\n",
    "lame_text = \"\"\n",
    "for paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n",
    "    lame_text += paragraph.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "'''\n",
    "Doing the same without removing stop words or lemming\n",
    "'''\n",
    "# tokenize the text using sent_tokenize\n",
    "\n",
    "# from this list of sentences, create a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence (after preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Redo the word cloud but set stopwords to empty so it looks really bad\n",
    "wordcloud = WordCloud(\n",
    "                        background_color='white',\n",
    "                        max_words=100,\n",
    "                        max_font_size=50, \n",
    "                        random_state=42, ###SET STOPWORDS = [] and/or include_numbers = True or you will get the same thing!!!\n",
    "                        stopwords = [],\n",
    "                        include_numbers = True).generate(str(lame_sentences)) \n",
    "fig = plt.figure(1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Training the Word2Vec model (same code as before), but one change: use our lame data that was not preprocessed\n",
    "\n",
    "# Try printing this after training the model.\n",
    "words = model.wv.index_to_key\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Finding a vector of a word, but badly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Finding the most similar words in the model but... you get the idea ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reflection\n",
    "How important do you think proper preprocessing in NLP is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "335ee12212264728feb72f243af72c5a8ea26c832f07e1f651ce9e17c7ceae23"
  },
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "nlp_env",
   "resource_dir": "/projects/0ddeade5-3577-4fe8-8cd6-8a0cb653428e/.local/share/jupyter/kernels/nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}